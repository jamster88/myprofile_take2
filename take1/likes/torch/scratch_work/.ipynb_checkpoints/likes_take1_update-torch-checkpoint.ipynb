{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e030b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]= \"expandable_segments:True\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "# torch.cuda.is_available = lambda : False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca107524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch backend.\n"
     ]
    }
   ],
   "source": [
    "import readline\n",
    "import scipy.sparse\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras_core.models import Sequential\n",
    "from keras_core.layers import Input, Dense, Dropout, Activation\n",
    "from keras_core.optimizers import RMSprop, Adam, Nadam, Adamax\n",
    "from keras_core.models import model_from_json\n",
    "# import sklearn.external.joblib as extjoblib\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f39952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'LIKES' from the training dataset and process\n",
    "###############\n",
    "likes = pd.read_csv(\"/home/jamster/old-repos/ml2018-projectDATA/tcss555/training/relation/relation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1ca714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# likes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40fb3829-8683-414f-a5df-df1fb0b8f61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'userid', 'like_id'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a35ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extact individual columns and convert to lists\n",
    "likesUIDs = likes['userid'].values\n",
    "likesLIDs = likes['like_id'].values\n",
    "lsLikesUIDs = likesUIDs.tolist()\n",
    "lsLikesLIDs = likesLIDs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50585e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to sets\n",
    "setLikesUIDs = set(lsLikesUIDs)\n",
    "setLikesLIDs = set(lsLikesLIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b2c7c99-2539-4af6-821e-e37826e91fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to list of unique items\n",
    "unqLikesUIDs = (list(setLikesUIDs))\n",
    "unqLikesLIDs = (list(setLikesLIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259d4348-53cc-4c78-9313-e0e8aad4231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all User IDs (UIDs) paried with the Like IDs (LIDs) of the \n",
    "#   posts the user has liked\n",
    "allLikesLS = [lsLikesUIDs, [str(x) for x in lsLikesLIDs]]\n",
    "allLikesLS = list(map(list, zip(*allLikesLS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b9641e-6c2f-4990-b970-939ccfd49165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of UID and LID pairs into a dictionary indexed by UIDs\n",
    "aDictLikes2 = {}\n",
    "for aUID in unqLikesUIDs:\n",
    "\taDictLikes2[aUID]=[]\n",
    "\n",
    "for row in allLikesLS:\n",
    "\taDictLikes2[row[0]].append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0284345a-7859-4c5f-9a22-0dc130d6e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into a dictionary (by UIDs) of dictionaries (by LIDs)\n",
    "combDICT = {}\n",
    "for uid in unqLikesUIDs:\n",
    "\ttmpDICT={}\n",
    "\ttmpLS = aDictLikes2[uid]\n",
    "\tfor row in tmpLS:\n",
    "\t\ttmpDICT[str(row)]=1\n",
    "\tcombDICT[uid]=tmpDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9ed975d-2b77-4c87-b31f-e6b8d0fe5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'combDICT' into a list of dictionaries (of LIDs)\n",
    "tryTHIS=[]\n",
    "for uid in unqLikesUIDs:\n",
    "\ttryTHIS.append(combDICT[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10716796-702c-4867-96d0-f48345accb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the list of dictionaries in 'tryTHIS' to get the UID/LID matrix \n",
    "#   for the training data\n",
    "v = DictVectorizer()\n",
    "likesMAT=v.fit_transform(tryTHIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a7ee0d6-6faf-4860-8e32-8e16129df67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear unused variable to free memory\n",
    "del globals()['likes']\n",
    "del globals()['likesUIDs']\n",
    "del globals()['likesLIDs']\n",
    "del globals()['lsLikesUIDs']\n",
    "del globals()['lsLikesLIDs']\n",
    "del globals()['setLikesUIDs']\n",
    "del globals()['setLikesLIDs']\n",
    "del globals()['allLikesLS']\n",
    "del globals()['aDictLikes2']\n",
    "del globals()['aUID']\n",
    "del globals()['row']\n",
    "del globals()['combDICT']\n",
    "del globals()['uid']\n",
    "del globals()['tmpDICT']\n",
    "del globals()['tmpLS']\n",
    "del globals()['tryTHIS']\n",
    "del globals()['v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d7d954e-25b4-4a41-b935-810db7854b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the profiles from the training dataset\n",
    "###############\n",
    "profilesDF=pd.read_csv(\"/home/jamster/old-repos/ml2018-projectDATA/tcss555/training/profile/profile.csv\")\n",
    "\n",
    "# profilesDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6d84afc-d6cc-45bc-b4f6-8129ca7f657f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'userid', 'age', 'gender', 'ope', 'con', 'ext', 'agr',\n",
       "       'neu'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profilesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6e7b349-e9fe-4af2-baba-e1c11a0ac68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values of the relevant columns and convert them to a list\n",
    "profiles=profilesDF[['userid', 'age', 'gender', 'ope', 'con', 'ext', 'agr', 'neu']].values.copy()\n",
    "profilesLSo=profiles.tolist().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "635f8d9d-b614-45d9-9267-c26e9b80a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the ages\n",
    "profilesLS=[]\n",
    "for row in profilesLSo:\n",
    "\ttmpLS=row\n",
    "\ttmpAGE=row[1]\n",
    "\n",
    "\tif tmpAGE < 25:\n",
    "\t\ttmpLS[1]=1\n",
    "\telif tmpAGE < 35:\n",
    "\t\ttmpLS[1]=2\n",
    "\telif tmpAGE < 50:\n",
    "\t\ttmpLS[1]=3\n",
    "\telse:\n",
    "\t\ttmpLS[1]=4\n",
    "\n",
    "\tprofilesLS.append(tmpLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f8bbb0a-a393-410b-b09e-986caa68dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the profiles data with the indexing of the likes data\n",
    "profsTOlikes=[]\n",
    "for i in range(len(profilesLS)):\n",
    "\tprofsTOlikes.append([])\n",
    "\n",
    "for row in profilesLS:\n",
    "\ttmpIND = unqLikesUIDs.index(row[0])\n",
    "\tprofsTOlikes[tmpIND]=row\n",
    "\n",
    "profsTOlikes1=list(map(list, zip(*profsTOlikes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b17e2d38-6299-4511-9c2f-abef47286b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data for AGEs\n",
    "agesARRo=np.array(profsTOlikes1[1])\n",
    "agesARRo=agesARRo.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68b035a8-ddb0-4a37-b139-4a547bc02696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data for AGEs to binary vectors\n",
    "agesARR = []\n",
    "for row in agesARRo:\n",
    "\tif row==1:\n",
    "\t\tagesARR.append([1,0,0,0])\n",
    "\telif row==2:\n",
    "\t\tagesARR.append([0,1,0,0])\n",
    "\telif row==3:\n",
    "\t\tagesARR.append([0,0,1,0])\n",
    "\telse:\n",
    "\t\tagesARR.append([0,0,0,1])\n",
    "\n",
    "agesARR=np.array(agesARR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "592ebe3a-dfcf-4fd2-b6b6-9fbe4b73831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear MORE unused variable to free memory\n",
    "del globals()['unqLikesUIDs']\n",
    "del globals()['unqLikesLIDs']\n",
    "del globals()['profilesDF']\n",
    "del globals()['profiles']\n",
    "del globals()['profilesLSo']\n",
    "del globals()['profilesLS']\n",
    "del globals()['row']\n",
    "del globals()['tmpLS']\n",
    "del globals()['tmpAGE']\n",
    "del globals()['profsTOlikes']\n",
    "del globals()['i']\n",
    "del globals()['tmpIND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a0ccbcc-dfa4-477f-816e-c25bf7d79482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "torch.get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92c6d472-c049-4186-94f4-2501c965008e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50b9d791-214b-4125-95bb-851a68845833",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff2ae18d-2e0b-4720-aa0b-ab2c3d82608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# torch.cuda.set_device(0)\n",
    "# torch.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d4e4d4e-463d-457d-992f-eea9cdd30d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_likesMAT, test_likesMAT, train_agesARR, test_agesARR = train_test_split(likesMAT, agesARR, test_size=0.05, random_state=42)\n",
    "train_likesMAT, test_likesMAT, train_agesARR, test_agesARR = train_test_split(likesMAT, agesARR, test_size=0.0525, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c915406-7403-40c6-8ff3-216fa21edc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9001, 536204)\n"
     ]
    }
   ],
   "source": [
    "print(train_likesMAT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb0f769c-1dd7-49a5-97eb-a024bca61294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# tensor_likesMAT = torch.Tensor(likesMAT.todense())#.to_sparse()\n",
    "# tensor_likesMAT = tensor_likesMAT.to(torch.float8_e4m3fn)\n",
    "# tensor_likesMAT = tensor_likesMAT.cuda()\n",
    "# tensor_agesARR = torch.Tensor(agesARR)\n",
    "# tensor_agesARR = tensor_agesARR.to(torch.bool)\n",
    "# tensor_agesARR = tensor_agesARR.cuda()\n",
    "\n",
    "tensor_train_likesMAT = torch.Tensor(train_likesMAT.todense())#.to_sparse()\n",
    "tensor_train_likesMAT = tensor_train_likesMAT.to(torch.torch.bfloat16)#.cuda()\n",
    "tensor_test_likesMAT = torch.Tensor(test_likesMAT.todense())#.to_sparse()\n",
    "tensor_test_likesMAT = tensor_test_likesMAT.to(torch.torch.bfloat16)\n",
    "\n",
    "tensor_train_agesARR = torch.Tensor(train_agesARR)\n",
    "tensor_train_agesARR = tensor_train_agesARR.to(torch.torch.bfloat16)#.cuda()\n",
    "tensor_test_agesARR = torch.Tensor(test_agesARR)\n",
    "tensor_test_agesARR = tensor_test_agesARR.to(torch.torch.bfloat16)\n",
    "\n",
    "# tensor_dataset = TensorDataset(tensor_likesMAT, tensor_agesARR)\n",
    "# tensor_loader = DataLoader(tensor_dataset, batch_size=128)\n",
    "# tensor_loader = DataLoader(tensor_dataset, batch_size=9500)\n",
    "\n",
    "tensor_train_dataset = TensorDataset(tensor_train_likesMAT, tensor_train_agesARR)\n",
    "tensor_train_loader = DataLoader(tensor_train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "tensor_test_dataset = TensorDataset(tensor_test_likesMAT, tensor_test_agesARR)\n",
    "tensor_test_loader = DataLoader(tensor_test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33668ec9-7581-4ecd-bc06-34944ad883dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2afa01c4-d207-4f3f-927f-f99f36d96d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as t_func\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(536204, 1125)\n",
    "        self.drop1 = nn.Dropout(p=0.25)\n",
    "        self.fc2 = nn.Linear(1125, 1500)#.to(device)\n",
    "        self.drop2 = nn.Dropout(p=0.375)#.to(device)\n",
    "        self.fc3 = nn.Linear(1500, 1125)#.to(device)\n",
    "        self.drop3 = nn.Dropout(p=0.25)#.to(device)\n",
    "        self.fc4 = nn.Linear(1125, 750)#.to(device)\n",
    "        self.fc5 = nn.Linear(750, 4)#.to(device)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = t_func.relu(x)\n",
    "        x = self.drop1(x)\n",
    "        x = x.to(device)\n",
    "        x = self.fc2(x)\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        # x = t_func.log_softmax(x)\n",
    "        x = sm(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = t_func.sigmoid(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = t_func.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = sm(x)\n",
    "        # x = t_func.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d64f0-0ff3-4ed9-944d-39149408fddb",
   "metadata": {},
   "source": [
    "Reference https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9e18c9b-d293-455b-b788-6a0c5e5b7e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.012\n",
      "[1,    40] loss: 0.011\n",
      "[1,    60] loss: 0.011\n",
      "[2,    20] loss: 0.011\n",
      "[2,    40] loss: 0.011\n",
      "[2,    60] loss: 0.011\n",
      "[3,    20] loss: 0.011\n",
      "[3,    40] loss: 0.012\n",
      "[3,    60] loss: 0.011\n",
      "[4,    20] loss: 0.011\n",
      "[4,    40] loss: 0.011\n",
      "[4,    60] loss: 0.011\n",
      "[5,    20] loss: 0.011\n",
      "[5,    40] loss: 0.011\n",
      "[5,    60] loss: 0.011\n",
      "[6,    20] loss: 0.011\n",
      "[6,    40] loss: 0.011\n",
      "[6,    60] loss: 0.011\n",
      "[7,    20] loss: 0.011\n",
      "[7,    40] loss: 0.011\n",
      "[7,    60] loss: 0.012\n",
      "[8,    20] loss: 0.011\n",
      "[8,    40] loss: 0.012\n",
      "[8,    60] loss: 0.011\n",
      "[9,    20] loss: 0.012\n",
      "[9,    40] loss: 0.011\n",
      "[9,    60] loss: 0.011\n",
      "[10,    20] loss: 0.011\n",
      "[10,    40] loss: 0.011\n",
      "[10,    60] loss: 0.011\n",
      "[11,    20] loss: 0.012\n",
      "[11,    40] loss: 0.011\n",
      "[11,    60] loss: 0.011\n",
      "[12,    20] loss: 0.012\n",
      "[12,    40] loss: 0.011\n",
      "[12,    60] loss: 0.011\n",
      "[13,    20] loss: 0.011\n",
      "[13,    40] loss: 0.011\n",
      "[13,    60] loss: 0.012\n",
      "[14,    20] loss: 0.011\n",
      "[14,    40] loss: 0.012\n",
      "[14,    60] loss: 0.012\n",
      "[15,    20] loss: 0.011\n",
      "[15,    40] loss: 0.011\n",
      "[15,    60] loss: 0.012\n",
      "[16,    20] loss: 0.011\n",
      "[16,    40] loss: 0.011\n",
      "[16,    60] loss: 0.012\n",
      "[17,    20] loss: 0.011\n",
      "[17,    40] loss: 0.011\n",
      "[17,    60] loss: 0.011\n",
      "[18,    20] loss: 0.012\n",
      "[18,    40] loss: 0.011\n",
      "[18,    60] loss: 0.012\n",
      "[19,    20] loss: 0.011\n",
      "[19,    40] loss: 0.011\n",
      "[19,    60] loss: 0.011\n",
      "[20,    20] loss: 0.011\n",
      "[20,    40] loss: 0.012\n",
      "[20,    60] loss: 0.011\n",
      "[21,    20] loss: 0.012\n",
      "[21,    40] loss: 0.011\n",
      "[21,    60] loss: 0.011\n",
      "[22,    20] loss: 0.011\n",
      "[22,    40] loss: 0.012\n",
      "[22,    60] loss: 0.011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# print(loss.item())\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m19\u001b[39m:    \u001b[38;5;66;03m# print every 2000 mini-batches\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "network = Net()\n",
    "network = network.to(torch.bfloat16).cuda()\n",
    "optimizer = optim.Adam(network.parameters())\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "n_epochs = 10\n",
    "losses = []\n",
    "counter = []\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(tensor_train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(torch.bfloat16).cuda()\n",
    "        labels = labels.to(torch.bfloat16).cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = network(inputs)\n",
    "        # outputs = outputs.cuda()\n",
    "        # loss = criterion(outputs.to(torch.device(\"cpu\")), labels.to(torch.device(\"cpu\")))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        # print(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            # print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 1:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5570ab-087a-4152-8864-47d5c9a71eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5dfdee-96bc-4181-8f91-7c5b20abb469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "network = Net()\n",
    "network = network.to(torch.bfloat16).cuda()\n",
    "optimizer = optim.Adam(network.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "losses = []\n",
    "counter = []\n",
    "\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_ind, (data, target) in enumerate(tensor_train_loader):\n",
    "        data = data.unsqueeze(1)\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target.long().cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_ind % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_ind * len(data), len(tensor_train_loader.dataset), \n",
    "                100.0 * batch_ind / len(tensor_train_loader), loss.item()))\n",
    "            losses.append(loss.item())\n",
    "            counter.append((batch_ind*64) + ((epoch-1)*len(tensor_train_loader.dataset)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e073589-12c6-4b51-b8e9-a57b80cd5a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [31,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [17,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [14,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [15,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [28,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [8,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [24,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [29,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [9,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [13,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [16,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [21,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 21\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_ind \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Epoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     24\u001b[0m         epoch, batch_ind \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;28mlen\u001b[39m(tensor_train_loader\u001b[38;5;241m.\u001b[39mdataset), \n\u001b[1;32m     25\u001b[0m         \u001b[38;5;241m100.0\u001b[39m \u001b[38;5;241m*\u001b[39m batch_ind \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(tensor_train_loader), loss\u001b[38;5;241m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/keras_torch/lib/python3.11/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/keras_torch/lib/python3.11/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/keras_torch/lib/python3.11/site-packages/torch/optim/adam.py:216\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    213\u001b[0m     state_steps: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     adam(\n\u001b[1;32m    227\u001b[0m         params_with_grad,\n\u001b[1;32m    228\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/keras_torch/lib/python3.11/site-packages/torch/optim/adam.py:156\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    146\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    147\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    148\u001b[0m         (),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m_get_scalar_dtype())\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    160\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    161\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    162\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs+1):\n",
    "    train(epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f58c4d7-5cf7-4a26-93ae-a84dfa75eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/layers/core/dense.py:73: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-07-29 18:09:35.214514: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-29 18:09:35.400420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-29 18:09:35.490235: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-29 18:09:35.511864: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-29 18:09:35.674471: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.11/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.11/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_8823/1630248790.py\", line 26, in <module>\n",
      "    model.fit(likesMAT, agesARR, epochs=10)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/utils/traceback_utils.py\", line 118, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/backend/torch/trainer.py\", line 241, in fit\n",
      "    epoch_iterator = EpochIterator(\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/trainers/epoch_iterator.py\", line 79, in __init__\n",
      "    elif tf.available and isinstance(x, tf.data.Dataset):\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/utils/module_utils.py\", line 16, in available\n",
      "    self.initialize()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/utils/module_utils.py\", line 24, in initialize\n",
      "    self.module = importlib.import_module(self.name)\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/__init__.py\", line 47, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import autograph\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/autograph/core/ag_ctx.py\", line 21, in <module>\n",
      "    from tensorflow.python.autograph.utils import ag_logging\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/autograph/utils/__init__.py\", line 17, in <module>\n",
      "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/autograph/utils/context_managers.py\", line 19, in <module>\n",
      "    from tensorflow.python.framework import ops\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\", line 46, in <module>\n",
      "    from tensorflow.python import pywrap_tfe\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/pywrap_tfe.py\", line 25, in <module>\n",
      "    from tensorflow.python._pywrap_tfe import *\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.11/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.11/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_8823/1630248790.py\", line 26, in <module>\n",
      "    model.fit(likesMAT, agesARR, epochs=10)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/utils/traceback_utils.py\", line 118, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/backend/torch/trainer.py\", line 241, in fit\n",
      "    epoch_iterator = EpochIterator(\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/trainers/epoch_iterator.py\", line 79, in __init__\n",
      "    elif tf.available and isinstance(x, tf.data.Dataset):\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/utils/module_utils.py\", line 16, in available\n",
      "    self.initialize()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/utils/module_utils.py\", line 24, in initialize\n",
      "    self.module = importlib.import_module(self.name)\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/__init__.py\", line 47, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import autograph\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/autograph/core/ag_ctx.py\", line 21, in <module>\n",
      "    from tensorflow.python.autograph.utils import ag_logging\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/autograph/utils/__init__.py\", line 17, in <module>\n",
      "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/autograph/utils/context_managers.py\", line 19, in <module>\n",
      "    from tensorflow.python.framework import ops\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\", line 49, in <module>\n",
      "    from tensorflow.python.client import pywrap_tf_session\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/client/pywrap_tf_session.py\", line 19, in <module>\n",
      "    from tensorflow.python.client._pywrap_tf_session import *\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.11/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.11/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_8823/1630248790.py\", line 26, in <module>\n",
      "    model.fit(likesMAT, agesARR, epochs=10)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/utils/traceback_utils.py\", line 118, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/backend/torch/trainer.py\", line 241, in fit\n",
      "    epoch_iterator = EpochIterator(\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/trainers/epoch_iterator.py\", line 79, in __init__\n",
      "    elif tf.available and isinstance(x, tf.data.Dataset):\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/utils/module_utils.py\", line 16, in available\n",
      "    self.initialize()\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/keras_core/src/utils/module_utils.py\", line 24, in initialize\n",
      "    self.module = importlib.import_module(self.name)\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/__init__.py\", line 47, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 11, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import distribute\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__.distribute import combinations\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.distribute.combinations import env # line: 456\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/distribute/combinations.py\", line 33, in <module>\n",
      "    from tensorflow.python.distribute import collective_all_reduce_strategy\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 25, in <module>\n",
      "    from tensorflow.python.distribute import cross_device_ops as cross_device_ops_lib\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 28, in <module>\n",
      "    from tensorflow.python.distribute import cross_device_utils\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/distribute/cross_device_utils.py\", line 22, in <module>\n",
      "    from tensorflow.python.distribute import values as value_lib\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/distribute/values.py\", line 23, in <module>\n",
      "    from tensorflow.python.distribute import distribute_lib\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 205, in <module>\n",
      "    from tensorflow.python.data.ops import dataset_ops\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.data import experimental\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/data/experimental/__init__.py\", line 98, in <module>\n",
      "    from tensorflow.python.data.experimental import service\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/data/experimental/service/__init__.py\", line 419, in <module>\n",
      "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 26, in <module>\n",
      "    from tensorflow.python.data.ops import dataset_ops\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 34, in <module>\n",
      "    from tensorflow.python.data.ops import iterator_ops\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 45, in <module>\n",
      "    from tensorflow.python.training.saver import BaseSaverBuilder\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/training/saver.py\", line 50, in <module>\n",
      "    from tensorflow.python.training import py_checkpoint_reader\n",
      "  File \"/home/jamster/keras_torch/lib/python3.11/site-packages/tensorflow/python/training/py_checkpoint_reader.py\", line 19, in <module>\n",
      "    from tensorflow.python.util._pywrap_checkpoint_reader import CheckpointReader\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "initialization of _pywrap_checkpoint_reader raised unreported exception",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 26\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m4\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m \t\t\t\tloss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m \t\t\t\tmetrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlikesMAT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magesARR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/keras_torch/lib/python3.11/site-packages/keras_core/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras_core.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1149\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/keras_torch/lib/python3.11/site-packages/tensorflow/__init__.py:47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     45\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m~/keras_torch/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "File \u001b[0;32m~/keras_torch/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[0;32m~/keras_torch/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "\u001b[0;31mSystemError\u001b[0m: initialization of _pywrap_checkpoint_reader raised unreported exception"
     ]
    }
   ],
   "source": [
    "# Training Model\n",
    "###############\n",
    "print(\"start training\")\n",
    "\n",
    "numInputs = 750 # Number of nodes to map inputs\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(int(numInputs*1.5),\n",
    "\t\t\t\tinput_dim=int(likesMAT.shape[1]),\n",
    "\t\t\t\tactivation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense((numInputs*2),\n",
    "\t\t\t\tactivation='softmax'))\n",
    "model.add(Dropout(0.375))\n",
    "model.add(Dense(int(numInputs*1.5),\n",
    "\t\t\t\tactivation='sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(numInputs,\n",
    "\t\t\t\tactivation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "\t\t\t\tloss='categorical_crossentropy',\n",
    "\t\t\t\tmetrics=['accuracy', 'mse'])\n",
    "\n",
    "model.fit(likesMAT, agesARR, epochs=10)\n",
    "\n",
    "print(\"end training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316420c-eaa3-4421-ac15-525bea899750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_torch",
   "language": "python",
   "name": "keras_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
